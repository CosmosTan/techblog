(window.webpackJsonp=window.webpackJsonp||[]).push([[22],{303:function(t,e,a){"use strict";a.r(e);var i=a(14),n=Object(i.a)({},(function(){var t=this,e=t._self._c;return e("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[e("h1",{attrs:{id:"神经网络"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#神经网络"}},[t._v("#")]),t._v(" 神经网络：")]),t._v(" "),e("p",[t._v("多层感知机（multi-layer perceptron, MLP）也叫作深度前馈网络（deep feedforward network）或前馈神经网络（feedforward neural network）， 它通过已有的信息或者知识来对未知事物进行预测．")]),t._v(" "),e("h2",{attrs:{id:"_1、dnn"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_1、dnn"}},[t._v("#")]),t._v(" 1、DNN")]),t._v(" "),e("h2",{attrs:{id:"二、基于梯度的学习"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#二、基于梯度的学习"}},[t._v("#")]),t._v(" 二、基于梯度的学习")]),t._v(" "),e("h3",{attrs:{id:"_1-代价函数"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_1-代价函数"}},[t._v("#")]),t._v(" （1）代价函数")]),t._v(" "),e("h3",{attrs:{id:"_2-输出单元形式"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_2-输出单元形式"}},[t._v("#")]),t._v(" （2）输出单元形式")]),t._v(" "),e("p",[t._v("❑ $o ∈ R^d$ 输出属于整个实数空间,或者某段普通的实数空间,比如函数值趋势的预测,年龄的预测问题等。"),e("strong",[t._v("输出层直接恒等映射")]),t._v("（可以不加激活函数）。误差的计算直接基于最后一层的输出o和真实值 y 进行计算,如采用均方差误差函数度量输出值o与真实值y之间的距离:"),e("br"),t._v("\n❑ $o ∈ [0,1]$ 输出值特别地落在[0, 1]的区间,如图片生成,图片像素值一般用[0, 1]表示;或者二分类问题的概率,如硬币正反面的概率预测问题"),e("strong",[t._v("Sigmoid！！")]),e("br"),t._v("\n❑ $o ∈ [0, 1]$, 输出值落在[0, 1]的区间,并且所有输出值之和为 1,常见的如多分类问题,如 MNIST 手写数字图片识别,图片属于 10 个类别的概率之和为 1。"),e("strong",[t._v("Softmax！！")]),e("br"),t._v("\n❑ $o ∈ [−1, 1]$ 输出值在[-1, 1]之间。"),e("strong",[t._v("tanh！！")])]),t._v(" "),e("h2",{attrs:{id:"三、隐藏层-隐藏单元"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#三、隐藏层-隐藏单元"}},[t._v("#")]),t._v(" 三、隐藏层，隐藏单元")]),t._v(" "),e("h3",{attrs:{id:"_1-激活函数"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_1-激活函数"}},[t._v("#")]),t._v(" （1）激活函数：")]),t._v(" "),e("h4",{attrs:{id:"_1-sigmoid-logistic"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_1-sigmoid-logistic"}},[t._v("#")]),t._v(" 1 sigmoid/logistic")]),t._v(" "),e("p",[e("a",{attrs:{href:"https://zhuanlan.zhihu.com/p/35697684",target:"_blank",rel:"noopener noreferrer"}},[t._v("sigmoid 和 softmax 函数区别"),e("OutboundLink")],1)]),t._v(" "),e("p",[t._v("![[Pasted image 20231026180844.png|300]]")]),t._v(" "),e("ul",[e("li",[t._v("当人们逐渐关注到到基于梯度的学习时， sigmoid函数是一个自然的选择，因为它是一个平滑的、可微的阈值单元近似。 当我们想要将输出视作二元分类问题的概率时， sigmoid仍然被广泛用作输出单元上的激活函数 （sigmoid可以视为softmax的特例）\n![[Pasted image 20230602020620.png]]")]),t._v(" "),e("li",[t._v("对于一个定义域在R中的输入， sigmoid函数将输入变换为区间(0, 1)上的输出。 因此，sigmoid通常称为_挤压函数_（squashing function）： 它将范围（-inf, inf）中的任意输入压缩到区间（0, 1）中的某个值：\n$$\\operatorname{sigmoid}(x) = \\frac{1}{1 + \\exp(-x)}.$$")]),t._v(" "),e("li",[t._v("sigmoid函数的导数："),e("strong",[t._v("当输入为0时，sigmoid函数的导数达到最大值0.25")]),t._v("； 而输入在任一方向上越远离0点时，导数越接近0。$$\\frac{d}{dx} \\operatorname{sigmoid}(x) = \\frac{\\exp(-x)}{(1 + \\exp(-x))^2} = \\operatorname{sigmoid}(x)\\left(1-\\operatorname{sigmoid}(x)\\right).$$\n![[Pasted image 20231026165525.png|500]]")])]),t._v(" "),e("p",[e("strong",[t._v("Sigmoid's limitations：")])]),t._v(" "),e("ul",[e("li",[t._v("在（-3， 3）以外区域，导数趋近于0，梯度消失问题。（"),e("em",[t._v("Vanishing gradient")]),t._v(" problem）")]),t._v(" "),e("li",[t._v("The output of the logistic function is not symmetric around zero. So the output of all the neurons will be of the same sign. This makes the "),e("a",{attrs:{href:"https://www.v7labs.com/training",target:"_blank",rel:"noopener noreferrer"}},[t._v("training of the neural network"),e("OutboundLink")],1),t._v(" more difficult and unstable")])]),t._v(" "),e("h4",{attrs:{id:"_2-tanh函数"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_2-tanh函数"}},[t._v("#")]),t._v(" 2 tanh函数：")]),t._v(" "),e("ul",[e("li",[t._v("与sigmoid函数类似， tanh(双曲正切)函数也能将其输入压缩转换到区间(-1, 1)上。 tanh函数的公式如下：$$\\operatorname{tanh}(x) = \\frac{1 - \\exp(-2x)}{1 + \\exp(-2x)}.$$")]),t._v(" "),e("li",[t._v("tanh函数导数：$$\\frac{d}{dx} \\operatorname{tanh}(x) = 1 - \\operatorname{tanh}^2(x).$$"),e("strong",[t._v("当输入接近0时，tanh函数的导数接近最大值1")]),t._v("。 与我们在sigmoid函数图像中看到的类似， 输入在任一方向上越远离0点，导数越接近0")])]),t._v(" "),e("p",[t._v("![[Pasted image 20230602021036.png]]")]),t._v(" "),e("h4",{attrs:{id:"_3-relu"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_3-relu"}},[t._v("#")]),t._v(" 3 RELU")]),t._v(" "),e("p",[e("em",[t._v("修正线性单元")]),t._v("（Rectified linear unit，"),e("em",[t._v("ReLU")]),t._v("）\n$$\\operatorname{ReLU}(x) = \\max(x, 0).$$\n求导表现得特别好："),e("strong",[t._v("要么让参数消失，要么让参数通过")]),t._v("。 这使得优化表现得更好，并且ReLU减轻了困扰以往神经网络的梯度消失问题。\n![[Pasted image 20230602020639.png]]")]),t._v(" "),e("p",[e("strong",[t._v("三者的主要区别")]),t._v("：")]),t._v(" "),e("ul",[e("li",[t._v("Sigmoid 型激活函数会导致一个非稀疏的神经网络，而 "),e("strong",[t._v("ReLU 却具有很好 的稀疏性")]),t._v("，大约50%的神经元会处于激活状态．")]),t._v(" "),e("li",[t._v("在优化方面，相比于Sigmoid型函数的两端饱和，ReLU函数为左饱和函数， "),e("strong",[t._v("且在 𝑥 > 0 时导数为 1，在一定程度上缓解了神经网络的梯度消失问题")]),t._v("，加速梯度下降的收敛速度．")])]),t._v(" "),e("blockquote",[e("p",[e("strong",[t._v("💡Note:")]),t._v("  Although both sigmoid and tanh face vanishing gradient issue, "),e("em",[t._v("tanh is zero centered, and the gradients are not restricted to move in a certain direction.")]),t._v(" Therefore, in practice, tanh nonlinearity is always preferred to sigmoid nonlinearity.")])]),t._v(" "),e("h4",{attrs:{id:"_4-others"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_4-others"}},[t._v("#")]),t._v(" 4 others")]),t._v(" "),e("table",[e("thead",[e("tr",[e("th",[t._v("Active function")]),t._v(" "),e("th",[t._v("Mathematical fomula")]),t._v(" "),e("th",[t._v("advantage")]),t._v(" "),e("th",[t._v("limitations")])])]),t._v(" "),e("tbody",[e("tr",[e("td",[t._v("Leaky ReLU")]),t._v(" "),e("td",[t._v("$f(x) = max(0.1x,x)$")]),t._v(" "),e("td"),t._v(" "),e("td")]),t._v(" "),e("tr",[e("td",[t._v("Parametric ReLU")]),t._v(" "),e("td",[t._v("$f(x) = max(ax,x)$")]),t._v(" "),e("td"),t._v(" "),e("td")]),t._v(" "),e("tr",[e("td",[t._v("ELU")]),t._v(" "),e("td",[t._v("$\\left{\\begin{array}{ll} x & \\text { for } x  \\geqslant 0 \\ \\alpha\\left(e^{x}-1\\right) & \\text {for } x<0\\end{array}\\right.$")]),t._v(" "),e("td"),t._v(" "),e("td")]),t._v(" "),e("tr",[e("td",[e("strong",[t._v("GELU")])]),t._v(" "),e("td",[t._v("$\\begin{array}{c}f(x)=x P(X \\leq x)=x \\Phi(x) \\=0.5 x\\left(1+\\tanh \\left[\\sqrt{2 / \\pi}\\left(x+0.044715x^{3}\\right)\\right]\\right)\\end{array}$")]),t._v(" "),e("td"),t._v(" "),e("td")]),t._v(" "),e("tr",[e("td",[t._v("SELU")]),t._v(" "),e("td",[t._v("![[Pasted image 20231026181024.png|300]]")]),t._v(" "),e("td"),t._v(" "),e("td")]),t._v(" "),e("tr",[e("td",[e("strong",[t._v("Swish")])]),t._v(" "),e("td",[t._v("$f(x) = x \\times sigmoid(x)$")]),t._v(" "),e("td"),t._v(" "),e("td")])])]),t._v(" "),e("p",[t._v("![[Pasted image 20231026174818.png]]")]),t._v(" "),e("h4",{attrs:{id:"_5-how-to-choose"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_5-how-to-choose"}},[t._v("#")]),t._v(" 5 how to choose")]),t._v(" "),e("p",[t._v("Here’s what you should keep in mind."),e("br"),t._v("\nAs a rule of thumb, you can begin with using the ReLU activation function and then move over to other activation functions if ReLU doesn’t provide optimum results.")]),t._v(" "),e("blockquote",[e("p",[t._v("And here are a few other guidelines to help you out.")])]),t._v(" "),e("ol",[e("li",[t._v("ReLU activation function should only be used in the hidden layers.")]),t._v(" "),e("li",[t._v("Sigmoid/Logistic and Tanh functions should not be used in hidden layers as they make the model more susceptible to problems during training (due to vanishing gradients).")]),t._v(" "),e("li",[t._v("Swish function is used in neural networks having a depth greater than 40 layers.")])]),t._v(" "),e("blockquote",[e("p",[t._v("Finally, a few rules for choosing the activation function for your output layer based on the type of prediction problem that you are solving:")])]),t._v(" "),e("ol",[e("li",[e("strong",[t._v("Regression")]),t._v(" - Linear Activation Function")]),t._v(" "),e("li",[e("strong",[t._v("Binary Classification")]),t._v("—Sigmoid/Logistic Activation Function")]),t._v(" "),e("li",[e("strong",[t._v("Multiclass Classification")]),t._v("—Softmax")]),t._v(" "),e("li",[e("strong",[t._v("Multilabel Classification")]),t._v("—Sigmoid"),e("br"),t._v("\nThe activation function used in hidden layers is typically chosen based on the type of neural network architecture.")]),t._v(" "),e("li",[e("strong",[t._v("Convolutional Neural Network (CNN)")]),t._v(": ReLU activation function.")]),t._v(" "),e("li",[e("a",{attrs:{href:"https://www.v7labs.com/blog/recurrent-neural-networks-guide",target:"_blank",rel:"noopener noreferrer"}},[e("strong",[t._v("Recurrent Neural Network")]),e("OutboundLink")],1),t._v(": Tanh and/or Sigmoid activation function.")])]),t._v(" "),e("p",[e("a",{attrs:{href:"https://www.v7labs.com/blog/neural-networks-activation-functions",target:"_blank",rel:"noopener noreferrer"}},[t._v("Activation Functions in Neural Networks"),e("OutboundLink")],1),t._v(" "),e("a",{attrs:{href:"https://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6",target:"_blank",rel:"noopener noreferrer"}},[t._v("Activation Functions in Neural Networks--SAGAR SHARMA"),e("OutboundLink")],1)]),t._v(" "),e("p",[e("a",{attrs:{href:"https://medium.com/analytics-vidhya/activation-functions-all-you-need-to-know-355a850d025e#id_token=eyJhbGciOiJSUzI1NiIsImtpZCI6ImEwNmFmMGI2OGEyMTE5ZDY5MmNhYzRhYmY0MTVmZjM3ODgxMzZmNjUiLCJ0eXAiOiJKV1QifQ.eyJpc3MiOiJodHRwczovL2FjY291bnRzLmdvb2dsZS5jb20iLCJhenAiOiIyMTYyOTYwMzU4MzQtazFrNnFlMDYwczJ0cDJhMmphbTRsamRjbXMwMHN0dGcuYXBwcy5nb29nbGV1c2VyY29udGVudC5jb20iLCJhdWQiOiIyMTYyOTYwMzU4MzQtazFrNnFlMDYwczJ0cDJhMmphbTRsamRjbXMwMHN0dGcuYXBwcy5nb29nbGV1c2VyY29udGVudC5jb20iLCJzdWIiOiIxMDQwMDU1Nzk2NzIzMzMwMDY0MzMiLCJlbWFpbCI6InRhbnFpbmd5dTJAZ21haWwuY29tIiwiZW1haWxfdmVyaWZpZWQiOnRydWUsIm5iZiI6MTY5ODMwOTcwOSwibmFtZSI6InFpbmd5dSB0YW4iLCJwaWN0dXJlIjoiaHR0cHM6Ly9saDMuZ29vZ2xldXNlcmNvbnRlbnQuY29tL2EvQUNnOG9jSnRqSVdpcUNtWlJXRWRzVWFFa1ozRFU3OUlOQ3ZPOEg3bnYzSllTbFdxVnc9czk2LWMiLCJnaXZlbl9uYW1lIjoicWluZ3l1IiwiZmFtaWx5X25hbWUiOiJ0YW4iLCJsb2NhbGUiOiJ6aC1DTiIsImlhdCI6MTY5ODMxMDAwOSwiZXhwIjoxNjk4MzEzNjA5LCJqdGkiOiJiOGM1M2VjNjA2ZjE4OWIxYjRjNTFlZGIyZTU4ZGZmYWIwZjQxNTMyIn0.sfevZViBh8cGTGjvJt6KhPkCYb3krkiIfvWBue1GWzi50eo-C1AgdBzOetZaFmwb5TOkru07VAFESRFYyzW4cTw1TojGI8m6QUozgdVCOsnzyDmYUSlXxKXH4UkAfxInUDxuEeBy0VTcVOvSAFah6rGAsOFyb8foDa1pg3pdbrOplTAx23NgwT5GF640qSIi8J_8AHtPpseO7sklSVCBat0ibFV33IpJ6Wh3RtyPOC6ZQkzbKP6L-_PDeDJ7XSUOm_ziiBXPHFr0IbqGOjc06WlLjt53VqDt9YYkSRwopaZFVazMZdjqVwp33U9q87qsdC2GFhWBUfeOGk46-yB7tw",target:"_blank",rel:"noopener noreferrer"}},[t._v("Activation Functions — All You Need To Know"),e("OutboundLink")],1),t._v("\n![[Pasted image 20231026172356.png]]")]),t._v(" "),e("h2",{attrs:{id:"四、反向传播算法"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#四、反向传播算法"}},[t._v("#")]),t._v(" 四、反向传播算法")]),t._v(" "),e("h4",{attrs:{id:"_1-梯度下降"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_1-梯度下降"}},[t._v("#")]),t._v(" 1 梯度下降")]),t._v(" "),e("h4",{attrs:{id:"_2-链式求导法则"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_2-链式求导法则"}},[t._v("#")]),t._v(" 2 链式求导法则")])])}),[],!1,null,null,null);e.default=n.exports}}]);