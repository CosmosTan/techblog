<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>神经网络： | Cosmos的个人博客</title>
    <meta name="generator" content="VuePress 1.9.10">
    <link rel="icon" href="/techblog/favicon.ico">
    <meta name="description" content="tqy的个人博客">
    
    <link rel="preload" href="/techblog/assets/css/0.styles.12ac2825.css" as="style"><link rel="preload" href="/techblog/assets/js/app.1d240a96.js" as="script"><link rel="preload" href="/techblog/assets/js/2.3dc1b8de.js" as="script"><link rel="preload" href="/techblog/assets/js/1.7f771cfb.js" as="script"><link rel="preload" href="/techblog/assets/js/22.70c1a97f.js" as="script"><link rel="prefetch" href="/techblog/assets/js/10.23a1f579.js"><link rel="prefetch" href="/techblog/assets/js/11.c389195a.js"><link rel="prefetch" href="/techblog/assets/js/12.1d996921.js"><link rel="prefetch" href="/techblog/assets/js/13.4d4410c4.js"><link rel="prefetch" href="/techblog/assets/js/14.37ef2a72.js"><link rel="prefetch" href="/techblog/assets/js/15.5542c093.js"><link rel="prefetch" href="/techblog/assets/js/16.d48fd1ce.js"><link rel="prefetch" href="/techblog/assets/js/17.bd8d538c.js"><link rel="prefetch" href="/techblog/assets/js/18.6d3b94c1.js"><link rel="prefetch" href="/techblog/assets/js/19.eb35cfee.js"><link rel="prefetch" href="/techblog/assets/js/20.c11ec329.js"><link rel="prefetch" href="/techblog/assets/js/21.db1b5d88.js"><link rel="prefetch" href="/techblog/assets/js/23.7f13beaf.js"><link rel="prefetch" href="/techblog/assets/js/24.6c0ce24d.js"><link rel="prefetch" href="/techblog/assets/js/25.bf96aed2.js"><link rel="prefetch" href="/techblog/assets/js/26.e5f683cd.js"><link rel="prefetch" href="/techblog/assets/js/27.28972161.js"><link rel="prefetch" href="/techblog/assets/js/28.1f668152.js"><link rel="prefetch" href="/techblog/assets/js/29.7b36a84c.js"><link rel="prefetch" href="/techblog/assets/js/3.5322f14a.js"><link rel="prefetch" href="/techblog/assets/js/4.84e1e480.js"><link rel="prefetch" href="/techblog/assets/js/5.f0541060.js"><link rel="prefetch" href="/techblog/assets/js/6.dfb06aa0.js"><link rel="prefetch" href="/techblog/assets/js/7.7551a9fb.js"><link rel="prefetch" href="/techblog/assets/js/vendors~docsearch.5e19b665.js">
    <link rel="stylesheet" href="/techblog/assets/css/0.styles.12ac2825.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div class="theme-container no-sidebar"><header class="navbar"><div class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/techblog/" class="home-link router-link-active"><img src="/techblog/img/logo.png" alt="Cosmos的个人博客" class="logo"> <span class="site-name can-hide">Cosmos的个人博客</span></a> <div class="links"><div class="search-box"><input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <nav class="nav-links can-hide"><div class="nav-item"><a href="/techblog/" class="nav-link">
  首页
</a></div><div class="nav-item"><a href="/techblog/Python/" class="nav-link">
  Python
</a></div><div class="nav-item"><a href="/techblog/Linux/" class="nav-link">
  Linux
</a></div><div class="nav-item"><a href="/techblog/ML/" class="nav-link">
  ML
</a></div><div class="nav-item"><a href="/techblog/DL/" class="nav-link router-link-active">
  DL
</a></div><div class="nav-item"><a href="https://github.com/CosmosTan/techblog.git/" target="_blank" rel="noopener noreferrer" class="nav-link external">
  github
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></div> <!----></nav></div></header> <div class="sidebar-mask"></div> <aside class="sidebar"><nav class="nav-links"><div class="nav-item"><a href="/techblog/" class="nav-link">
  首页
</a></div><div class="nav-item"><a href="/techblog/Python/" class="nav-link">
  Python
</a></div><div class="nav-item"><a href="/techblog/Linux/" class="nav-link">
  Linux
</a></div><div class="nav-item"><a href="/techblog/ML/" class="nav-link">
  ML
</a></div><div class="nav-item"><a href="/techblog/DL/" class="nav-link router-link-active">
  DL
</a></div><div class="nav-item"><a href="https://github.com/CosmosTan/techblog.git/" target="_blank" rel="noopener noreferrer" class="nav-link external">
  github
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></div> <!----></nav>  <!----> </aside> <main class="page"> <div class="theme-default-content content__default"><h1 id="神经网络"><a href="#神经网络" class="header-anchor">#</a> 神经网络：</h1> <p>多层感知机（multi-layer perceptron, MLP）也叫作深度前馈网络（deep feedforward network）或前馈神经网络（feedforward neural network）， 它通过已有的信息或者知识来对未知事物进行预测．</p> <h2 id="_1、dnn"><a href="#_1、dnn" class="header-anchor">#</a> 1、DNN</h2> <h2 id="二、基于梯度的学习"><a href="#二、基于梯度的学习" class="header-anchor">#</a> 二、基于梯度的学习</h2> <h3 id="_1-代价函数"><a href="#_1-代价函数" class="header-anchor">#</a> （1）代价函数</h3> <h3 id="_2-输出单元形式"><a href="#_2-输出单元形式" class="header-anchor">#</a> （2）输出单元形式</h3> <p>❑ $o ∈ R^d$ 输出属于整个实数空间,或者某段普通的实数空间,比如函数值趋势的预测,年龄的预测问题等。<strong>输出层直接恒等映射</strong>（可以不加激活函数）。误差的计算直接基于最后一层的输出o和真实值 y 进行计算,如采用均方差误差函数度量输出值o与真实值y之间的距离:<br>
❑ $o ∈ [0,1]$ 输出值特别地落在[0, 1]的区间,如图片生成,图片像素值一般用[0, 1]表示;或者二分类问题的概率,如硬币正反面的概率预测问题<strong>Sigmoid！！</strong><br>
❑ $o ∈ [0, 1]$, 输出值落在[0, 1]的区间,并且所有输出值之和为 1,常见的如多分类问题,如 MNIST 手写数字图片识别,图片属于 10 个类别的概率之和为 1。<strong>Softmax！！</strong><br>
❑ $o ∈ [−1, 1]$ 输出值在[-1, 1]之间。<strong>tanh！！</strong></p> <h2 id="三、隐藏层-隐藏单元"><a href="#三、隐藏层-隐藏单元" class="header-anchor">#</a> 三、隐藏层，隐藏单元</h2> <h3 id="_1-激活函数"><a href="#_1-激活函数" class="header-anchor">#</a> （1）激活函数：</h3> <h4 id="_1-sigmoid-logistic"><a href="#_1-sigmoid-logistic" class="header-anchor">#</a> 1 sigmoid/logistic</h4> <p><a href="https://zhuanlan.zhihu.com/p/35697684" target="_blank" rel="noopener noreferrer">sigmoid 和 softmax 函数区别<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></p> <p>![[Pasted image 20231026180844.png|300]]</p> <ul><li>当人们逐渐关注到到基于梯度的学习时， sigmoid函数是一个自然的选择，因为它是一个平滑的、可微的阈值单元近似。 当我们想要将输出视作二元分类问题的概率时， sigmoid仍然被广泛用作输出单元上的激活函数 （sigmoid可以视为softmax的特例）
![[Pasted image 20230602020620.png]]</li> <li>对于一个定义域在R中的输入， sigmoid函数将输入变换为区间(0, 1)上的输出。 因此，sigmoid通常称为_挤压函数_（squashing function）： 它将范围（-inf, inf）中的任意输入压缩到区间（0, 1）中的某个值：
$$\operatorname{sigmoid}(x) = \frac{1}{1 + \exp(-x)}.$$</li> <li>sigmoid函数的导数：<strong>当输入为0时，sigmoid函数的导数达到最大值0.25</strong>； 而输入在任一方向上越远离0点时，导数越接近0。$$\frac{d}{dx} \operatorname{sigmoid}(x) = \frac{\exp(-x)}{(1 + \exp(-x))^2} = \operatorname{sigmoid}(x)\left(1-\operatorname{sigmoid}(x)\right).$$
![[Pasted image 20231026165525.png|500]]</li></ul> <p><strong>Sigmoid's limitations：</strong></p> <ul><li>在（-3， 3）以外区域，导数趋近于0，梯度消失问题。（<em>Vanishing gradient</em> problem）</li> <li>The output of the logistic function is not symmetric around zero. So the output of all the neurons will be of the same sign. This makes the <a href="https://www.v7labs.com/training" target="_blank" rel="noopener noreferrer">training of the neural network<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a> more difficult and unstable</li></ul> <h4 id="_2-tanh函数"><a href="#_2-tanh函数" class="header-anchor">#</a> 2 tanh函数：</h4> <ul><li>与sigmoid函数类似， tanh(双曲正切)函数也能将其输入压缩转换到区间(-1, 1)上。 tanh函数的公式如下：$$\operatorname{tanh}(x) = \frac{1 - \exp(-2x)}{1 + \exp(-2x)}.$$</li> <li>tanh函数导数：$$\frac{d}{dx} \operatorname{tanh}(x) = 1 - \operatorname{tanh}^2(x).$$<strong>当输入接近0时，tanh函数的导数接近最大值1</strong>。 与我们在sigmoid函数图像中看到的类似， 输入在任一方向上越远离0点，导数越接近0</li></ul> <p>![[Pasted image 20230602021036.png]]</p> <h4 id="_3-relu"><a href="#_3-relu" class="header-anchor">#</a> 3 RELU</h4> <p><em>修正线性单元</em>（Rectified linear unit，<em>ReLU</em>）
$$\operatorname{ReLU}(x) = \max(x, 0).$$
求导表现得特别好：<strong>要么让参数消失，要么让参数通过</strong>。 这使得优化表现得更好，并且ReLU减轻了困扰以往神经网络的梯度消失问题。
![[Pasted image 20230602020639.png]]</p> <p><strong>三者的主要区别</strong>：</p> <ul><li>Sigmoid 型激活函数会导致一个非稀疏的神经网络，而 <strong>ReLU 却具有很好 的稀疏性</strong>，大约50%的神经元会处于激活状态．</li> <li>在优化方面，相比于Sigmoid型函数的两端饱和，ReLU函数为左饱和函数， <strong>且在 𝑥 &gt; 0 时导数为 1，在一定程度上缓解了神经网络的梯度消失问题</strong>，加速梯度下降的收敛速度．</li></ul> <blockquote><p><strong>💡Note:</strong>  Although both sigmoid and tanh face vanishing gradient issue, <em>tanh is zero centered, and the gradients are not restricted to move in a certain direction.</em> Therefore, in practice, tanh nonlinearity is always preferred to sigmoid nonlinearity.</p></blockquote> <h4 id="_4-others"><a href="#_4-others" class="header-anchor">#</a> 4 others</h4> <table><thead><tr><th>Active function</th> <th>Mathematical fomula</th> <th>advantage</th> <th>limitations</th></tr></thead> <tbody><tr><td>Leaky ReLU</td> <td>$f(x) = max(0.1x,x)$</td> <td></td> <td></td></tr> <tr><td>Parametric ReLU</td> <td>$f(x) = max(ax,x)$</td> <td></td> <td></td></tr> <tr><td>ELU</td> <td>$\left{\begin{array}{ll} x &amp; \text { for } x  \geqslant 0 \ \alpha\left(e^{x}-1\right) &amp; \text {for } x&lt;0\end{array}\right.$</td> <td></td> <td></td></tr> <tr><td><strong>GELU</strong></td> <td>$\begin{array}{c}f(x)=x P(X \leq x)=x \Phi(x) \=0.5 x\left(1+\tanh \left[\sqrt{2 / \pi}\left(x+0.044715x^{3}\right)\right]\right)\end{array}$</td> <td></td> <td></td></tr> <tr><td>SELU</td> <td>![[Pasted image 20231026181024.png|300]]</td> <td></td> <td></td></tr> <tr><td><strong>Swish</strong></td> <td>$f(x) = x \times sigmoid(x)$</td> <td></td> <td></td></tr></tbody></table> <p>![[Pasted image 20231026174818.png]]</p> <h4 id="_5-how-to-choose"><a href="#_5-how-to-choose" class="header-anchor">#</a> 5 how to choose</h4> <p>Here’s what you should keep in mind.<br>
As a rule of thumb, you can begin with using the ReLU activation function and then move over to other activation functions if ReLU doesn’t provide optimum results.</p> <blockquote><p>And here are a few other guidelines to help you out.</p></blockquote> <ol><li>ReLU activation function should only be used in the hidden layers.</li> <li>Sigmoid/Logistic and Tanh functions should not be used in hidden layers as they make the model more susceptible to problems during training (due to vanishing gradients).</li> <li>Swish function is used in neural networks having a depth greater than 40 layers.</li></ol> <blockquote><p>Finally, a few rules for choosing the activation function for your output layer based on the type of prediction problem that you are solving:</p></blockquote> <ol><li><strong>Regression</strong> - Linear Activation Function</li> <li><strong>Binary Classification</strong>—Sigmoid/Logistic Activation Function</li> <li><strong>Multiclass Classification</strong>—Softmax</li> <li><strong>Multilabel Classification</strong>—Sigmoid<br>
The activation function used in hidden layers is typically chosen based on the type of neural network architecture.</li> <li><strong>Convolutional Neural Network (CNN)</strong>: ReLU activation function.</li> <li><a href="https://www.v7labs.com/blog/recurrent-neural-networks-guide" target="_blank" rel="noopener noreferrer"><strong>Recurrent Neural Network</strong><span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>: Tanh and/or Sigmoid activation function.</li></ol> <p><a href="https://www.v7labs.com/blog/neural-networks-activation-functions" target="_blank" rel="noopener noreferrer">Activation Functions in Neural Networks<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a> <a href="https://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6" target="_blank" rel="noopener noreferrer">Activation Functions in Neural Networks--SAGAR SHARMA<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></p> <p><a href="https://medium.com/analytics-vidhya/activation-functions-all-you-need-to-know-355a850d025e#id_token=eyJhbGciOiJSUzI1NiIsImtpZCI6ImEwNmFmMGI2OGEyMTE5ZDY5MmNhYzRhYmY0MTVmZjM3ODgxMzZmNjUiLCJ0eXAiOiJKV1QifQ.eyJpc3MiOiJodHRwczovL2FjY291bnRzLmdvb2dsZS5jb20iLCJhenAiOiIyMTYyOTYwMzU4MzQtazFrNnFlMDYwczJ0cDJhMmphbTRsamRjbXMwMHN0dGcuYXBwcy5nb29nbGV1c2VyY29udGVudC5jb20iLCJhdWQiOiIyMTYyOTYwMzU4MzQtazFrNnFlMDYwczJ0cDJhMmphbTRsamRjbXMwMHN0dGcuYXBwcy5nb29nbGV1c2VyY29udGVudC5jb20iLCJzdWIiOiIxMDQwMDU1Nzk2NzIzMzMwMDY0MzMiLCJlbWFpbCI6InRhbnFpbmd5dTJAZ21haWwuY29tIiwiZW1haWxfdmVyaWZpZWQiOnRydWUsIm5iZiI6MTY5ODMwOTcwOSwibmFtZSI6InFpbmd5dSB0YW4iLCJwaWN0dXJlIjoiaHR0cHM6Ly9saDMuZ29vZ2xldXNlcmNvbnRlbnQuY29tL2EvQUNnOG9jSnRqSVdpcUNtWlJXRWRzVWFFa1ozRFU3OUlOQ3ZPOEg3bnYzSllTbFdxVnc9czk2LWMiLCJnaXZlbl9uYW1lIjoicWluZ3l1IiwiZmFtaWx5X25hbWUiOiJ0YW4iLCJsb2NhbGUiOiJ6aC1DTiIsImlhdCI6MTY5ODMxMDAwOSwiZXhwIjoxNjk4MzEzNjA5LCJqdGkiOiJiOGM1M2VjNjA2ZjE4OWIxYjRjNTFlZGIyZTU4ZGZmYWIwZjQxNTMyIn0.sfevZViBh8cGTGjvJt6KhPkCYb3krkiIfvWBue1GWzi50eo-C1AgdBzOetZaFmwb5TOkru07VAFESRFYyzW4cTw1TojGI8m6QUozgdVCOsnzyDmYUSlXxKXH4UkAfxInUDxuEeBy0VTcVOvSAFah6rGAsOFyb8foDa1pg3pdbrOplTAx23NgwT5GF640qSIi8J_8AHtPpseO7sklSVCBat0ibFV33IpJ6Wh3RtyPOC6ZQkzbKP6L-_PDeDJ7XSUOm_ziiBXPHFr0IbqGOjc06WlLjt53VqDt9YYkSRwopaZFVazMZdjqVwp33U9q87qsdC2GFhWBUfeOGk46-yB7tw" target="_blank" rel="noopener noreferrer">Activation Functions — All You Need To Know<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>
![[Pasted image 20231026172356.png]]</p> <h2 id="四、反向传播算法"><a href="#四、反向传播算法" class="header-anchor">#</a> 四、反向传播算法</h2> <h4 id="_1-梯度下降"><a href="#_1-梯度下降" class="header-anchor">#</a> 1 梯度下降</h4> <h4 id="_2-链式求导法则"><a href="#_2-链式求导法则" class="header-anchor">#</a> 2 链式求导法则</h4></div> <footer class="page-edit"><!----> <!----></footer> <!----> </main></div><div class="global-ui"></div></div>
    <script src="/techblog/assets/js/app.1d240a96.js" defer></script><script src="/techblog/assets/js/2.3dc1b8de.js" defer></script><script src="/techblog/assets/js/1.7f771cfb.js" defer></script><script src="/techblog/assets/js/22.70c1a97f.js" defer></script>
  </body>
</html>
